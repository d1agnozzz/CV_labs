{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.,  2., 10.],\n",
       "          [ 8.,  2., 10.],\n",
       "          [10.,  1.,  1.]],\n",
       "\n",
       "         [[ 3.,  6.,  4.],\n",
       "          [ 2.,  3.,  1.],\n",
       "          [ 9.,  6., 10.]],\n",
       "\n",
       "         [[ 6.,  2., 10.],\n",
       "          [ 4.,  4.,  5.],\n",
       "          [ 9.,  4.,  2.]]]])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = 2\n",
    "kern = 3\n",
    "stride = 1\n",
    "dilation = 1\n",
    "padding = 0\n",
    "bias = bias=torch.Tensor([1, 2, 3])\n",
    "\n",
    "inputs = torch.randint(1,11,(2, 1, inp, inp)).float()\n",
    "filters = torch.randint(1, 11, (1, 3, kern, kern)).float()\n",
    "\n",
    "conv = F.conv_transpose2d(inputs, filters, padding=padding, dilation=dilation, stride=stride, bias=bias)\n",
    "print(conv.shape)\n",
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from math import floor\n",
    "\n",
    "def dim_window_count(input, weights, stride, padding, dilation, dim):\n",
    "    return floor(\n",
    "        (\n",
    "            input.shape[2 + dim]\n",
    "            + 2 * padding[dim]\n",
    "            - dilation[dim] * (weights.shape[2 + dim] - 1)\n",
    "            - 1\n",
    "        )\n",
    "        / stride[dim]\n",
    "        + 1\n",
    "    )\n",
    "\n",
    "def custom_conv2d(\n",
    "    input: Tensor,\n",
    "    weights: Tensor,\n",
    "    bias: Tensor | None = None,\n",
    "    stride: tuple | int = 1,\n",
    "    padding: int | str | tuple[int, int] = 0,\n",
    "    dilation: tuple | int = 1,\n",
    "    groups: int = 1,\n",
    "):\n",
    "    if len(input.shape) < 3 or len(input.shape) > 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: {input.shape}\"\n",
    "        )\n",
    "    if len(input.shape) == 3:\n",
    "        input = input.unsqueeze(0)\n",
    "        print(f\"unsqueezed input: {input.shape}\")\n",
    "\n",
    "    if len(weights.shape) != 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected 4D weights with shape [out_channels, in_channels/groups, kH, kW], but got weights with shape: {weights.shape}\"\n",
    "        )\n",
    "\n",
    "    if bias is not None and bias.shape[0] != weights.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Expected bias shape to be [out_channels], but got bias with shape: {bias.shape}\"\n",
    "        )\n",
    "\n",
    "    batch_size, input_channels, input_h, input_w = input.shape\n",
    "    output_channels, kernel_channels, kernel_h, kernel_w = weights.shape\n",
    "\n",
    "    if input_channels % groups != 0 or output_channels % groups != 0:\n",
    "        raise ValueError(\n",
    "            f\"Groups should be divisible by both input_channels and output_channels. Got:\\ninput channels: {input_channels}\\noutput channels: {output_channels}\\ngroups: {groups}\"\n",
    "        )\n",
    "    if kernel_channels != input_channels // groups:\n",
    "        raise ValueError(\n",
    "            f\"Expected kernel channels to be [input_channels/groups], got:\\n\\\n",
    "                input channels: {input_channels}\\n\\\n",
    "                groups: {groups}\\n\\\n",
    "                output channels: {output_channels}\"\n",
    "        )\n",
    "    if type(dilation) is int:\n",
    "        dilation = (dilation, dilation)\n",
    "    if type(padding) is int:\n",
    "        padding = (padding, padding)\n",
    "    if type(stride) is int:\n",
    "        stride = (stride, stride)\n",
    "    if type(padding) is int:\n",
    "        padding = (padding, padding)\n",
    "\n",
    "    print(padding)\n",
    "\n",
    "    # input_paded = F.pad(\n",
    "    #     input, (padding[0], padding[0], padding[1], padding[1]), \"constant\", 0\n",
    "    # )\n",
    "\n",
    "    output_h = dim_window_count(input, weights, stride, padding, dilation, 0)\n",
    "    output_w = dim_window_count(input, weights, stride, padding, dilation, 1)\n",
    "    output = torch.zeros(size=(batch_size, output_channels, output_h, output_w))\n",
    "\n",
    "    windows_per_input_channel = output_h*output_w\n",
    "\n",
    "    unfolded = F.unfold(input, (kernel_h, kernel_w), dilation, padding, stride)\n",
    "\n",
    "    for img_num, img in enumerate(input):\n",
    "        for kernel_num, kernel in enumerate(weights):\n",
    "            group_num = kernel_num // (output_channels // groups)\n",
    "            input_channels_start = group_num * (input_channels // groups)\n",
    "            for input_ch_num in range(\n",
    "                input_channels_start, input_channels_start + input_channels // groups\n",
    "            ):\n",
    "                # for kernel_ch_num, kernel_ch in enumerate(kernel):\n",
    "                kernel_ch_num = input_ch_num % kernel_channels\n",
    "                # print(\n",
    "                #     f\"kernel channel {kernel_ch_num} applied to input channel {input_ch_num} to output {kernel_num}\"\n",
    "                # )\n",
    "\n",
    "\n",
    "                kernel_area = kernel_h * kernel_w\n",
    "                channel_windows_start = input_ch_num * kernel_area\n",
    "\n",
    "                folds = unfolded[\n",
    "                    img_num,\n",
    "                    channel_windows_start : channel_windows_start + kernel_area,\n",
    "                ]\n",
    "                current_input_channel_windows = folds.T.reshape(windows_per_input_channel, kernel_h, kernel_w)\n",
    "                # print(img[input_ch_num])\n",
    "                # print(windows)\n",
    "                product = current_input_channel_windows * kernel[kernel_ch_num]\n",
    "                # print(f'p: {product[:3]}')\n",
    "                weighted_sum = torch.sum(product, dim=(1, 2), keepdim=True)\n",
    "                # print(f'ws: {weighted_sum.shape}')\n",
    "\n",
    "                weighted_sum = weighted_sum.reshape(output_h, output_w)\n",
    "\n",
    "                output[img_num, kernel_num] += weighted_sum\n",
    "\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_conv_transpose2d(    \n",
    "    input: Tensor,\n",
    "    weights: Tensor,\n",
    "    bias: Tensor | None = None,\n",
    "    stride: tuple | int = 1,\n",
    "    padding: int | str | tuple[int, int] = 0,\n",
    ") :\n",
    "    if len(input.shape) < 3 or len(input.shape) > 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: {input.shape}\"\n",
    "        )\n",
    "    if len(input.shape) == 3:\n",
    "        input = input.unsqueeze(0)\n",
    "        print(f\"unsqueezed input: {input.shape}\")\n",
    "\n",
    "    if len(weights.shape) != 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected 4D weights with shape [out_channels, in_channels/groups, kH, kW], but got weights with shape: {weights.shape}\"\n",
    "        )\n",
    "\n",
    "    if bias is not None and bias.shape[0] != weights.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Expected bias shape to be [out_channels], but got bias with shape: {bias.shape}\"\n",
    "        )\n",
    "\n",
    "    batch_size, input_channels, input_h, input_w = input.shape\n",
    "    input_channels, out_channels, kernel_h, kernel_w = weights.shape\n",
    "\n",
    "    if type(stride) is int:\n",
    "        stride = (stride, stride)\n",
    "    if type(padding) is int:\n",
    "        padding = (padding, padding)\n",
    "\n",
    "    z = tuple(map(lambda x: x-1, stride))\n",
    "    p_ = tuple(map(lambda t: t[0] - t[1] - 1, zip(weights.shape[2:], padding)))\n",
    "\n",
    "    expanded_h = input_h + (input_h-1) * (z[0])\n",
    "    expanded_w = input_w + (input_w-1) * (z[1])\n",
    "    expanded = torch.zeros((batch_size, input_channels, expanded_h, expanded_w))\n",
    "    expanded[::, ::, ::z[0]+1, ::z[1]+1] = input[::, ::, ::, ::]\n",
    "\n",
    "    expanded_padded = Tensor(np.pad(expanded, ((0,0), (0,0), (p_[0], p_[0]), (p_[1], p_[1]))))\n",
    "\n",
    "    print(f'expanded inp: {expanded_padded}')\n",
    "\n",
    "    result = torch.zeros((batch_size, out_channels, expanded_padded.shape[2]-kernel_h+1, expanded_padded.shape[3]-kernel_w+1))\n",
    "\n",
    "    for img_num, img in enumerate(result):\n",
    "        for ch_num, _ in enumerate(img):\n",
    "            for k_num, _ in enumerate(expanded_padded[img_num:img_num+1]):\n",
    "                result[img_num:img_num+1, ch_num:ch_num+1] += custom_conv2d(\n",
    "                    expanded_padded[img_num:img_num+1, k_num:k_num+1],\n",
    "                    weights[k_num:k_num+1, ch_num:ch_num+1, ::, ::].flip(3,2),\n",
    "                    stride=1,\n",
    "                    bias=bias[ch_num:ch_num+1],\n",
    "                )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_1():\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    bias = torch.Tensor([1])\n",
    "    inputs = torch.rand((2, 1, 2, 2)).float()\n",
    "    filters = torch.rand((1, 1, 3, 3)).float()\n",
    "\n",
    "    conv = F.conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    my_conv = custom_conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    assert torch.allclose(conv, my_conv)\n",
    "\n",
    "def test_2():\n",
    "    stride = 2\n",
    "    padding = 0\n",
    "    bias = torch.Tensor([2])\n",
    "    inputs = torch.rand((2, 1, 3, 3)).float()\n",
    "    filters = torch.rand((1, 1, 5, 5)).float()\n",
    "\n",
    "    conv = F.conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    my_conv = custom_conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    assert torch.allclose(conv, my_conv)\n",
    "\n",
    "def test_3():\n",
    "    stride = 3\n",
    "    padding = 1\n",
    "    bias = torch.Tensor([4])\n",
    "    inputs = torch.rand((2, 1, 8, 8)).float()\n",
    "    filters = torch.rand((1, 1, 4, 4)).float()\n",
    "\n",
    "    conv = F.conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    my_conv = custom_conv_transpose2d(inputs, filters, stride=stride, padding=padding, bias=bias)\n",
    "    assert torch.allclose(conv, my_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
